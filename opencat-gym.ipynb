{"cells":[{"cell_type":"markdown","metadata":{"id":"_H3kIRihf3Ma"},"source":["# Mounting Drive\n","A data drive is mounted, from where the URDF-model will be loaded. It is also necessary to save training data."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24912,"status":"ok","timestamp":1706975681526,"user":{"displayName":"Gerold S.","userId":"08847029510990248057"},"user_tz":-60},"id":"nNWNZdhMQub2","outputId":"86afae1a-903a-4780-8fdf-c5df1fb0c4da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"8YhlWsZBgPOd"},"source":["# Install Python Packages\n","Now the python packages needed are installed. **Stable Baselines** provides the training agents and suits as a framework.\n","\n","**Pybullet** is the simulation environment. Into Pybullet the robot model URDF will be loaded and serves as a training environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWaEe1mgZ2jv"},"outputs":[],"source":["!pip install \"stable-baselines3[extra]\"\n","!pip install pybullet\n","#!pip install sb3-contrib\n","#!pip install sbx-rl"]},{"cell_type":"markdown","metadata":{"id":"sIP1lDbihCGE"},"source":["# Gymnasium Training Environment\n","Here we describe the training environment as a **gymnasium** environment."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":256,"status":"ok","timestamp":1706982719446,"user":{"displayName":"Gerold S.","userId":"08847029510990248057"},"user_tz":-60},"id":"doNTS_gKZVKk"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import pybullet as p\n","import pybullet_data\n","\n","\n","# Constants to define training and visualisation.\n","GUI_MODE = False          # Set \"True\" to display pybullet in a window\n","EPISODE_LENGTH = 250      # Number of steps for one training episode\n","MAXIMUM_LENGTH = 2e6      # Number of total steps for entire training\n","\n","# Factors to weight rewards and penalties.\n","PENALTY_STEPS = 2e6       # Increase of penalty by step_counter/PENALTY_STEPS\n","FAC_MOVEMENT = 1000.0     # Reward movement in x-direction\n","FAC_STABILITY = 0.1       # Punish body roll and pitch velocities\n","FAC_Z_VELOCITY = 0.0      # Punish z movement of body\n","FAC_SLIP = 0.0            # Punish slipping of paws\n","FAC_ARM_CONTACT = 0.01    # Punish crawling on arms and elbows\n","FAC_SMOOTH_1 = 1.0        # Punish jitter and vibrational movement, 1st order\n","FAC_SMOOTH_2 = 1.0        # Punish jitter and vibrational movement, 2nd order\n","FAC_CLEARANCE = 0.0       # Factor to enfore foot clearance to PAW_Z_TARGET\n","PAW_Z_TARGET = 0.005      # Target height (m) of paw during swing phase\n","\n","BOUND_ANGLE = 110         # Joint maximum angle (deg)\n","STEP_ANGLE = 11           # Maximum angle (deg) delta per step\n","ANG_FACTOR = 0.1          # Improve angular velocity resolution before clip.\n","\n","# Values for randomization, to improve sim to real transfer.\n","RANDOM_GYRO = 0           # Percent\n","RANDOM_joint_angs = 0      # Percent\n","RANDOM_MASS = 0           # Percent, currently inactive\n","RANDOM_FRICTION = 0       # Percent, currently inactive\n","\n","LENGTH_RECENT_ANGLES = 3  # Buffer to read recent joint angles\n","LENGTH_JOINT_HISTORY = 30 # Number of steps to store joint angles.\n","\n","# Size of oberservation space is set up of:\n","# [LENGTH_JOINT_HISTORY, quaternion, gyro]\n","SIZE_OBSERVATION = LENGTH_JOINT_HISTORY * 8 + 6\n","\n","\n","class OpenCatGymEnv(gym.Env):\n","    \"\"\" Gymnasium environment (stable baselines 3) for OpenCat robots.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human']}\n","\n","    def __init__(self):\n","        self.step_counter = 0\n","        self.step_counter_session = 0\n","        self.state_history = np.array([])\n","        self.angle_history = np.array([])\n","        self.bound_ang = np.deg2rad(BOUND_ANGLE)\n","\n","        if GUI_MODE:\n","            p.connect(p.GUI)\n","            # Uncommend to create a video.\n","            #video_options = (\"--width=960 --height=540\n","            #                + \"--mp4=\\\"training.mp4\\\" --mp4fps=60\")\n","            #p.connect(p.GUI, options=video_options)\n","        else:\n","            # Use for training without visualisation (significantly faster).\n","            p.connect(p.DIRECT)\n","\n","        p.configureDebugVisualizer(p.COV_ENABLE_GUI, 0)\n","        p.resetDebugVisualizerCamera(cameraDistance=0.5,\n","                                     cameraYaw=-170,\n","                                     cameraPitch=-40,\n","                                     cameraTargetPosition=[0.4,0,0])\n","\n","        # The action space are the 8 joint angles.\n","        self.action_space = gym.spaces.Box(np.array([-1]*8), np.array([1]*8))\n","\n","        # The observation space are the torso roll, pitch and the\n","        # angular velocities and a history of the last 30 joint angles.\n","        self.observation_space = gym.spaces.Box(np.array([-1]*SIZE_OBSERVATION),\n","                                                np.array([1]*SIZE_OBSERVATION))\n","\n","\n","    def step(self, action):\n","        p.configureDebugVisualizer(p.COV_ENABLE_SINGLE_STEP_RENDERING)\n","        last_position = p.getBasePositionAndOrientation(self.robot_id)[0][0]\n","        joint_angs = np.asarray(p.getJointStates(self.robot_id, self.joint_id),\n","                                                   dtype=object)[:,0]\n","        ds = np.deg2rad(STEP_ANGLE) # Maximum change of angle per step\n","        joint_angs += action * ds # Change per step including agent action\n","\n","        # Apply joint boundaries individually.\n","        min_ang = -self.bound_ang\n","        max_ang = self.bound_ang\n","        joint_angs[0] = np.clip(joint_angs[0], min_ang, max_ang) # shoulder_left\n","        joint_angs[1] = np.clip(joint_angs[1], min_ang, max_ang) # elbow_left\n","        joint_angs[2] = np.clip(joint_angs[2], min_ang, max_ang) # shoulder_right\n","        joint_angs[3] = np.clip(joint_angs[3], min_ang, max_ang) # elbow_right\n","        joint_angs[4] = np.clip(joint_angs[4], min_ang, max_ang) # hip_right\n","        joint_angs[5] = np.clip(joint_angs[5], min_ang, max_ang) # knee_right\n","        joint_angs[6] = np.clip(joint_angs[6], min_ang, max_ang) # hip_left\n","        joint_angs[7] = np.clip(joint_angs[7], min_ang, max_ang) # knee_left\n","\n","        # Transform angle to degree and perform rounding, because\n","        # OpenCat robot have only integer values.\n","        joint_angsDeg = np.rad2deg(joint_angs.astype(np.float64))\n","        joint_angsDegRounded = joint_angsDeg.round()\n","        joint_angs = np.deg2rad(joint_angsDegRounded)\n","\n","        # Simulate delay for data transfer. Delay has to be modeled to close\n","        # \"reality gap\").\n","        p.stepSimulation()\n","\n","        # Check for friction of paws, to prevent slipping while training.\n","        paw_contact = []\n","        paw_idx = [3, 6, 9, 12]\n","        for idx in paw_idx:\n","            paw_contact.append(True if p.getContactPoints(bodyA=self.robot_id,\n","                                                          linkIndexA=idx)\n","                                    else False)\n","\n","        paw_slipping = 0\n","        for in_contact in np.nonzero(paw_contact)[0]:\n","            paw_slipping += np.linalg.norm((\n","                            p.getLinkState(self.robot_id,\n","                                           linkIndex=paw_idx[in_contact],\n","                                           computeLinkVelocity=1)[0][0:1]))\n","\n","        # Read clearance of paw from ground\n","        paw_clearance = 0\n","        for idx in paw_idx:\n","            paw_z_pos = p.getLinkState(self.robot_id, linkIndex=idx)[0][2]\n","            paw_clearance += (paw_z_pos-PAW_Z_TARGET)**2 * np.linalg.norm(\n","                (p.getLinkState(self.robot_id, linkIndex=idx,\n","                                computeLinkVelocity=1)[0][0:1]))**0.5\n","\n","        # Check if elbows or lower arm are in contact with ground\n","        arm_idx = [1, 2, 4, 5]\n","        for idx in arm_idx:\n","            if p.getContactPoints(bodyA=self.robot_id, linkIndexA=idx):\n","                self.arm_contact += 1\n","\n","        # Read clearance of torso from ground\n","        base_clearance = p.getBasePositionAndOrientation(self.robot_id)[0][2]\n","\n","        # Set new joint angles\n","        p.setJointMotorControlArray(self.robot_id,\n","                                    self.joint_id,\n","                                    p.POSITION_CONTROL,\n","                                    joint_angs,\n","                                    forces=np.ones(8)*0.2)\n","        p.stepSimulation() # Delay of data transfer\n","\n","        # Normalize joint_angs\n","        joint_angs[0] /= self.bound_ang\n","        joint_angs[1] /= self.bound_ang\n","        joint_angs[2] /= self.bound_ang\n","        joint_angs[3] /= self.bound_ang\n","        joint_angs[4] /= self.bound_ang\n","        joint_angs[5] /= self.bound_ang\n","        joint_angs[6] /= self.bound_ang\n","        joint_angs[7] /= self.bound_ang\n","\n","        # Adding every 2nd angle to the joint angle history.\n","        if(self.step_counter % 2 == 0):\n","            self.angle_history = np.append(self.angle_history,\n","                                           self.randomize(joint_angs,\n","                                                          RANDOM_joint_angs))\n","            self.angle_history = np.delete(self.angle_history, np.s_[0:8])\n","\n","        self.recent_angles = np.append(self.recent_angles, joint_angs)\n","        self.recent_angles = np.delete(self.recent_angles, np.s_[0:8])\n","\n","        joint_angs_prev = self.recent_angles[8:16]\n","        joint_angs_prev_prev = self.recent_angles[0:8]\n","\n","        # Read robot state (pitch, roll and their derivatives of the torso).\n","        state_pos, state_ang = p.getBasePositionAndOrientation(self.robot_id)\n","        p.stepSimulation() # Emulated delay of data transfer via serial port\n","        state_ang_euler = np.asarray(p.getEulerFromQuaternion(state_ang)[0:2])\n","        state_vel = np.asarray(p.getBaseVelocity(self.robot_id)[1])\n","        state_vel = state_vel[0:2]*ANG_FACTOR\n","        state_vel_clip = np.clip(state_vel, -1, 1)\n","        self.state_robot = np.concatenate((state_ang, state_vel_clip))\n","        current_position = p.getBasePositionAndOrientation(self.robot_id)[0][0]\n","\n","        # Penalty and reward\n","        smooth_movement = np.sum(\n","            FAC_SMOOTH_1*np.abs(joint_angs-joint_angs_prev)**2\n","            + FAC_SMOOTH_2*np.abs(joint_angs\n","            - 2*joint_angs_prev\n","            + joint_angs_prev_prev)**2)\n","\n","        z_velocity = p.getBaseVelocity(self.robot_id)[0][2]\n","\n","        body_stability = (FAC_STABILITY * (state_vel_clip[0]**2\n","                                          + state_vel_clip[1]**2)\n","                                          + FAC_Z_VELOCITY * z_velocity**2)\n","\n","        movement_forward = current_position - last_position\n","        reward = (FAC_MOVEMENT * movement_forward\n","                 - self.step_counter_session/PENALTY_STEPS * (\n","                    smooth_movement + body_stability\n","                    + FAC_CLEARANCE * paw_clearance\n","                    + FAC_SLIP * paw_slipping**2\n","                    + FAC_ARM_CONTACT * self.arm_contact))\n","\n","        # Set state of the current state.\n","        terminated = False\n","        truncated = False\n","        info = {}\n","\n","        # Stop criteria of current learning episode:\n","        # Number of steps or robot fell.\n","        self.step_counter += 1\n","        if self.step_counter > EPISODE_LENGTH:\n","            self.step_counter_session += self.step_counter\n","            terminated = False\n","            truncated = True\n","\n","        elif self.is_fallen(): # Robot fell\n","            self.step_counter_session += self.step_counter\n","            reward = 0\n","            terminated = True\n","            truncated = False\n","\n","        self.observation = np.hstack((self.state_robot, self.angle_history))\n","\n","        return (np.array(self.observation).astype(np.float32),\n","                        reward, terminated, truncated, info)\n","\n","\n","    def reset(self, seed=None, options=None):\n","        self.step_counter = 0\n","        self.arm_contact = 0\n","        p.resetSimulation()\n","        # Disable rendering during loading.\n","        p.configureDebugVisualizer(p.COV_ENABLE_RENDERING,0)\n","        p.setGravity(0,0,-9.81)\n","        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n","        plane_id = p.loadURDF(\"plane.urdf\")\n","\n","        start_pos = [0,0,0.08]\n","        start_orient = p.getQuaternionFromEuler([0,0,0])\n","\n","        urdf_path = \"/content/drive/My Drive/opencat-gym-esp32/models/\"\n","        self.robot_id = p.loadURDF(urdf_path + \"bittle_accurate.urdf\",\n","                                   start_pos, start_orient,\n","                                   flags=p.URDF_USE_SELF_COLLISION)\n","\n","        # Initialize urdf links and joints.\n","        self.joint_id = []\n","        #paramIds = []\n","        for j in range(p.getNumJoints(self.robot_id)):\n","            info = p.getJointInfo(self.robot_id, j)\n","            joint_name = info[1]\n","            joint_type = info[2]\n","\n","            if (joint_type == p.JOINT_PRISMATIC\n","                or joint_type == p.JOINT_REVOLUTE):\n","                self.joint_id.append(j)\n","                #paramIds.append(p.addUserDebugParameter(joint_name.decode(\"utf-8\")))\n","                # Limiting motor dynamics. Although bittle's dynamics seem to\n","                # be be quite high like up to 7 rad/s.\n","                p.changeDynamics(self.robot_id, j, maxJointVelocity = np.pi*10)\n","\n","        # Setting start position. This influences training.\n","        joint_angs = np.deg2rad(np.array([1, 0, 1, 0, 1, 0, 1, 0])*50)\n","\n","        i = 0\n","        for j in self.joint_id:\n","            p.resetJointState(self.robot_id,j, joint_angs[i])\n","            i = i+1\n","\n","        # Normalize joint angles.\n","        joint_angs[0] /= self.bound_ang\n","        joint_angs[1] /= self.bound_ang\n","        joint_angs[2] /= self.bound_ang\n","        joint_angs[3] /= self.bound_ang\n","        joint_angs[4] /= self.bound_ang\n","        joint_angs[5] /= self.bound_ang\n","        joint_angs[6] /= self.bound_ang\n","        joint_angs[7] /= self.bound_ang\n","\n","        # Read robot state (pitch, roll and their derivatives of the torso)\n","        state_ang = p.getBasePositionAndOrientation(self.robot_id)[1]\n","        state_vel = np.asarray(p.getBaseVelocity(self.robot_id)[1])\n","        state_vel = state_vel[0:2]*ANG_FACTOR\n","        self.state_robot = np.concatenate((state_ang,\n","                                           np.clip(state_vel, -1, 1)))\n","\n","        # Initialize robot state history with reset position\n","        state_joints = np.asarray(\n","            p.getJointStates(self.robot_id, self.joint_id), dtype=object)[:,0]\n","        state_joints /= self.bound_ang\n","\n","        self.angle_history = np.tile(state_joints, LENGTH_JOINT_HISTORY)\n","        self.recent_angles = np.tile(state_joints, LENGTH_RECENT_ANGLES)\n","        self.observation = np.concatenate((self.state_robot,\n","                                           self.angle_history))\n","        p.configureDebugVisualizer(p.COV_ENABLE_RENDERING,1)\n","        info = {}\n","        return np.array(self.observation).astype(np.float32), info\n","\n","\n","    def render(self, mode='human'):\n","        pass\n","\n","\n","    def close(self):\n","        p.disconnect()\n","\n","\n","    def is_fallen(self):\n","        \"\"\" Check if robot is fallen. It becomes \"True\",\n","            when pitch or roll is more than 1.3 rad.\n","        \"\"\"\n","        pos, orient = p.getBasePositionAndOrientation(self.robot_id)\n","        orient = p.getEulerFromQuaternion(orient)\n","        is_fallen = (np.fabs(orient[0]) > 1.3\n","                    or np.fabs(orient[1]) > 1.3)\n","\n","        return is_fallen\n","\n","\n","    def randomize(self, value, percentage):\n","        \"\"\" Randomize value within percentage boundaries.\n","        \"\"\"\n","        percentage /= 100\n","        value_randomized = value * (1 + percentage*(2*np.random.rand()-1))\n","\n","        return value_randomized"]},{"cell_type":"markdown","metadata":{"id":"h3SFtFMLhSL8"},"source":["#Starting Training\n","This is the main function to start training."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52AQWfpRZak9","outputId":"28226945-ca74-41cf-83a3-6ce9bcf07c76","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 251         |\n","|    ep_rew_mean          | 311         |\n","| time/                   |             |\n","|    fps                  | 307         |\n","|    iterations           | 27          |\n","|    time_elapsed         | 1436        |\n","|    total_timesteps      | 442368      |\n","| train/                  |             |\n","|    approx_kl            | 0.028403444 |\n","|    clip_fraction        | 0.322       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -11.2       |\n","|    explained_variance   | 0.846       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.93        |\n","|    n_updates            | 260         |\n","|    policy_gradient_loss | -0.00154    |\n","|    std                  | 0.985       |\n","|    value_loss           | 11.5        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 250        |\n","|    ep_rew_mean          | 311        |\n","| time/                   |            |\n","|    fps                  | 308        |\n","|    iterations           | 28         |\n","|    time_elapsed         | 1488       |\n","|    total_timesteps      | 458752     |\n","| train/                  |            |\n","|    approx_kl            | 0.02777461 |\n","|    clip_fraction        | 0.324      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -11.2      |\n","|    explained_variance   | 0.861      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 3.25       |\n","|    n_updates            | 270        |\n","|    policy_gradient_loss | -0.00329   |\n","|    std                  | 0.985      |\n","|    value_loss           | 12.6       |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 248        |\n","|    ep_rew_mean          | 321        |\n","| time/                   |            |\n","|    fps                  | 308        |\n","|    iterations           | 29         |\n","|    time_elapsed         | 1541       |\n","|    total_timesteps      | 475136     |\n","| train/                  |            |\n","|    approx_kl            | 0.03144688 |\n","|    clip_fraction        | 0.333      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -11.2      |\n","|    explained_variance   | 0.824      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.53       |\n","|    n_updates            | 280        |\n","|    policy_gradient_loss | -0.00429   |\n","|    std                  | 0.986      |\n","|    value_loss           | 13.6       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 250         |\n","|    ep_rew_mean          | 331         |\n","| time/                   |             |\n","|    fps                  | 308         |\n","|    iterations           | 30          |\n","|    time_elapsed         | 1595        |\n","|    total_timesteps      | 491520      |\n","| train/                  |             |\n","|    approx_kl            | 0.032277133 |\n","|    clip_fraction        | 0.34        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -11.2       |\n","|    explained_variance   | 0.82        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.54        |\n","|    n_updates            | 290         |\n","|    policy_gradient_loss | -0.00579    |\n","|    std                  | 0.989       |\n","|    value_loss           | 12.4        |\n","-----------------------------------------\n"]}],"source":["from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n","\n","# Create OpenCatGym environment from class and check if structure is correct\n","env = OpenCatGymEnv()\n","check_env(env)\n","\n","if __name__ == \"__main__\":\n","    # Set up number of parallel environments\n","    parallel_envs = 8\n","    envs = make_vec_env(OpenCatGymEnv, n_envs=parallel_envs, vec_env_cls=SubprocVecEnv) # only for PPO\n","\n","    # Change architecture of neural network to two hidden layers of size 256\n","    custom_arch = dict(net_arch=[256, 256])\n","\n","    # Define PPO agent and train\n","    model = PPO('MlpPolicy', envs,\n","                seed=42,\n","                policy_kwargs=custom_arch,\n","                n_steps=int(2048*8/parallel_envs),\n","                verbose=1).learn(MAXIMUM_LENGTH)\n","\n","    model.save(\"/content/drive/My Drive/opencat-gym-esp32/trained/opencat_gym_release_var3_seed_1p0_jitter_2e6\")\n","\n","    # Load model to continue previous training\n","    #model = PPO.load(\"/content/drive/My Drive/opencat-gym-esp32/trained/ppo_8_env_0p5_penalty_gradient_1p0_smooth_0p1_body_0p01_contact_arch_256_256_250steps_2e6\", env, policy_kwargs=custom_policy_kwargs, n_steps=int(2048*8/parallel_env), verbose=1, tensorboard_log=\"/content/drive/My Drive/opencat-gym-esp32/trained/tensorboard_logs/\").learn(2e6)\n","    #model.save(\"/content/drive/My Drive/opencat-gym-esp32/trained/ppo_8_env_0p5_penalty_gradient_1p0_smooth_0p1_body_0p01_contact_arch_256_256_250steps_4e6\")\n","\n","\n"]}],"metadata":{"colab":{"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyNbltI9LB7U1mdDpYBHOhHl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}